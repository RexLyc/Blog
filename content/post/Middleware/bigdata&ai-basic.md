---
title: "大数据和AI技术：基础篇"
date: 2022-11-16T10:18:39+08:00
categories:
- 计算机科学与技术
- 人工智能
tags:
- 人工智能
- 暂停更新
thumbnailImagePosition: left
thumbnailImage: /images/thumbnail/bigdata&ai.jpg
math: true
---
本章介绍一些基础、经典的应用，来总结一些常见的技术和术语。
<!--more-->
## 常规流程
1. 加载数据，做一定的可视化
1. 对数据进行预处理，重构维度，归一化等
1. 创建模型、编译模型
1. 使用数据训练模型
1. 测试
## 术语
### 深度学习
1. 神经网络（Neuron Network）：
    1. 卷积神经网络（CNN）：目前计算机视觉中最重要的一类神经网络。
    1. 循环神经网络（RNN）：包含一些循环，能够持久保留一些输入数据的信息
    1. 长短期记忆网络（LSTM）：
1. 层：
    1. 全连接层：又称为密集层（Dense），该层的每一个节点都会连接前序层的所有节点
    1. 卷积层：卷积神经网络最重要的一类层，使用卷积运算提取图像信息，主要是图片内细节部分的邻域信息，卷积可以该边输入数据大小、通道数量
        - 卷积核（Kernel）：卷积运算中所使用的内核。在CNN中，内核的各个值就是要训练的参数之一。
        - 步长（stride）：卷积核移动的距离
    1. 池化层（Pooling）：整合不同区域的邻域特征，更类似于采样的一个过程。池化层一般是**没有参数**需要训练的，另外池化也不会改变通道数量。一般的，池化和卷积交替出现。
        - 平均池化：取区域内的平均值
        - 最大池化：取区域内的最大值
    1. flattern层：将多维数据展平为一维数组，从此数据被称为特征向量，并进一步流向后续的分类层
    1. Dropout层：防止过拟合的一种技术，随机丢弃一部分神经元的数据，这一部分神经元将不进行前向推理和后向更新。使得整个网络不再过度依赖于某一部分神经元的数据。
    1. 归一化层：对数据进行归一化
1. 激活函数（激励函数）：一般用来提供非线性映射能力
    1. ReLU：
    1. Sigmoid
    1. SoftMax：用于多分类问题，输出结果是总和为1的若干分类概率
    1. TanH函数
1. 其他处理：
    1. 归一化：让训练更快（目标范围只在0~1之间），缩小不同维度数据的范围差异，提高泛化性能，另外不做归一化实际上也没法输入到激活函数上。
        1. 输入归一化：在输入时对数据先做一次归一化。但由于神经网络多为多层结构，数据可能在中途又变成较大的范围，造成协变量偏移（分布可能仍然是相似的，但是有偏移），为了让输入数据的分布仍然尽量一致，只做输入归一化并不够。
        1. 批量归一化：在单个神经元的上进行归一化。计算**该神经元对于一系列输入**的输出结果，计算其均值和方差，做**缩放和偏移**（这就是要学习的两个参数），进行归一化。
        1. 层归一化：批量归一化依赖于大量的训练样本。有时并不能满足这个要求。曾归一化是对**一个层的全部神经元对单一输入数据**的输出结果，计算其均值和方差，进行归一化。
        1. 参考：
            - [深度解析Batch normalization（批归一化）](https://zhuanlan.zhihu.com/p/435507061)
    1. 分类编码：对于分类问题，直接使用标签数字作为训练数据并不正确，因为不同数值在计算损失函数时会造成权重不公平，转为向量化的分类编码能够解决这个问题。比如独热编码（One Hot）
    1. 可视化内核：对内核的权重进行可视化处理
    1. 填充（padding）：卷积会造成输入数据大小变小，可以通过填充边界来避免这个问题。
    1. 数据增强：输入数据经常不会包含一些常见的变化，比如旋转、翻转、缩放、偏移、亮度变化等，对数据进行增强并填充到数据集中，能够进一步提升模型的泛化性
1. 参数量和计算量：
    1. 一般的卷积部分占20%参数量和80%的计算量
    1. 全连接部分占80%的参数量和20%的计算量
    1. Sigmoid等核的计算量比ReLU大得多
### 最优化
1. 梯度：损失函数减少最多的方向
1. 学习率：每一次权重更新移动的距离（系数）
1. 步：对权重进行一次更新（一步并不一定完成全数据集的计算，可能每输入一小部分数据集就进行一次更新）
1. 优化器：
    1. Adam
    1. SGD
    1. Adagrad
    1. RMSProp
1. 损失函数（Loss）：
    1. 多分类交叉熵
    1. 二分类交叉熵
    1. MSE、RMSE
1. 技术问题：
    1. 梯度爆炸
    1. 梯度消失

## 强化学习

参考：[动手学强化学习](https://hrl.boyuai.com/chapter/1/%E5%88%9D%E6%8E%A2%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0)

## 经典应用
### 手写数字识别
1. MNIST数据集

### 字母手势识别

### ImageNet
1. VGG

### 迁移学习
1. 原理：利用一个预训练模型，使用与原始任务存在部分重叠的新任务对其进行重新训练。新任务因此并不需要大量的数据，而且可以专注于某一种特定任务。
1. 基本步骤
    1. 导入预训练模型A，并将其冻结
    1. 为A添加适合新任务的输入层和输出层，以及必要的隐藏层
    1. 准备训练数据
    1. 编译、训练、确认收敛
    1. 微调（可选）：
        1. 解冻A，继续训练

### RNN语句生成
